{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "from datasets.load import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb_dataset = load_dataset('imdb', split=['train', 'test']) # Get the dataset from huggingface library\n",
    "train_imdb_dataset, test_imdb_dataset = torchtext.datasets.IMDB() # Get the dataset from torchtext library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparativos como los embeddings, el vocabulario, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import GloVe, FastText, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 100 # Dimension of the embeddings\n",
    "glove = GloVe(name='6B', dim=embeddings_dim) # Load GloVe embeddings with 100 dimensions.\n",
    "# fasttext = FastText(language='en') # To use FastText instead of GloVe\n",
    "vocabulary = vocab(glove.stoi)\n",
    "# vocabulary_fasttext = vocab(fasttext.stoi) # To use FastText instead of GloVe\n",
    "vocab_size = len(vocabulary) # Get the vocabulary size\n",
    "print(f\"Total vocabulary size: {vocab_size}\")\n",
    "print(f\"Shape of embeddings: {glove.vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"This is an example sentence to test the tokenizer.\"\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "spacy_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "example_tokens = tokenizer(example)\n",
    "example_tokens_spacy = spacy_tokenizer(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary.get_itos()[:10] # Get the first 10 words of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Padding token idx, pad: {vocabulary.get_stoi()['pad']}\") # Get the index of the word 'pad' for padding\n",
    "print(f\"Unknown token idx, unk: {vocabulary.get_stoi()['unk']}\") # Get the index of the word 'unk' for unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the basic english tokenizer from PyTorch, or the SpaCy tokenizer if we have spacy downloaded. Here we probe both tokenizer with the same example sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Basic English Tokenizer: {example_tokens}\")\n",
    "print(f\"Spacy Tokenizer: {example_tokens_spacy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Client's will probably want to delete the stopwords, optional, as the embeddings may have vectors for most of the stopwords. Here we show multiple options show the user must decide what he prefers to use. In this notebook we're going to use the first case, as it will have most information. In other case, we would use the last one, so at least we keep the most information we can. \n",
    "\n",
    "Later we will have to tokenize the clients data, and then we will add the padding to the sequences, and will convert the token to the index of the embedding matrix (ids)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(f\"Example tokens tokenized: {[word.lower() for word in example_tokens_spacy]}\")\n",
    "\n",
    "print(f\"Example tokens without stopwords: {[word.lower() for word in example_tokens_spacy if word not in stop_words]}\")\n",
    "\n",
    "print(f\"Example tokens without stopwords and word in vocabulary: {[word.lower() for word in example_tokens_spacy if word not in stop_words and word.lower() in vocabulary]}\")\n",
    "\n",
    "print(f\"Example tokens without quitting stopwords and word in vocabulary: {[word.lower() for word in example_tokens_spacy if word.lower() in vocabulary]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From centralized data to federated data\n",
    "\n",
    "First we're going to federate the dataset using the FedDataDristibution class, that has functions to load multiple datasets from deep learning libraries such as PyTorch or TensorFlow. In this notebook we are using PyTorch, so we need to use the functions from the PyTorch ecosystem, and for the text datasets, we need to use the function `from_config_with_torchtext_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.data import FedDatasetConfig, FedDataDistribution\n",
    "\n",
    "config = FedDatasetConfig(seed=0)\n",
    "config.n_clients = 2\n",
    "config.replacement = False # ensure that clients do not share any data\n",
    "config.client_names = ['client1', 'client2'] # Optional\n",
    "flex_dataset = FedDataDistribution.from_config_with_torchtext_dataset(data=train_imdb_dataset, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also want to use the FLEXible dataset for the test data, so we just use da function `from_torchtext_dataset` in the Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.data import Dataset\n",
    "\n",
    "test_dataset = Dataset.from_torchtext_dataset(test_imdb_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Federate a model with FLEXible.\n",
    "\n",
    "Once we've federated the dataset, it's time to create the FlexPool. The FlexPool class is the one that simulates the real-time scenario for federated learning, so it is in charge of the communications across actors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.model import FlexModel\n",
    "from flex.pool import FlexPool\n",
    "\n",
    "from flex.pool.decorators import init_server_model\n",
    "from flex.pool.decorators import deploy_server_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to simulate a client-server architecture, which we can easily build using the FlexPool class, using the function `client_server_architecture`. This function needs a FlexDataset, which we already have prepared, and a function to initialize the server model, which we have to create.\n",
    "\n",
    "The model we are going to use is a simple LSTM, which will have the embeddings, the LSTM, a Linear layer and the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes, emb_vectors) -> None:\n",
    "        super().__init__()\n",
    "        # self.emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embeddings_dim)\n",
    "        # self.emb.weight.data.copy_(emb_vectors) # Initialize the embedding layer with the pretrained embeddings\n",
    "        # self.emb.requires_grad_(False) # Freeze the embedding layer\n",
    "        # We can do the 3 steps above with the following line\n",
    "        self.emb = nn.Embedding.from_pretrained(embeddings=emb_vectors, freeze=True)\n",
    "        embeddings_dim = emb_vectors.shape[1]\n",
    "        self.lstm = nn.LSTM(input_size=embeddings_dim, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.fc2 = nn.Linear(hidden_size//2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f\"Showing the shape of the input: {x.shape}\")\n",
    "        x = self.emb(x)\n",
    "        # print(f\"Showing the shape of the input after the embedding layer: {x.shape}\")\n",
    "        x, _ = self.lstm(x)\n",
    "        x = F.relu(x[:, -1, :])\n",
    "        # print(f\"Showing the shape of the input after the LSTM layer: {x.shape}\")\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # print(f\"Showing the shape of the input after the first linear layer: {x.shape}\")\n",
    "        x = self.fc2(x)\n",
    "        # print(f\"Showing the shape of the output shape: {x.shape}\")\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "@init_server_model\n",
    "def build_server_model():\n",
    "    server_flex_model = FlexModel()\n",
    "\n",
    "    server_flex_model['model'] = LSTMNet(hidden_size=128, num_classes=2, emb_vectors=glove.vectors)\n",
    "    # Required to store this for later stages of the FL training process\n",
    "    server_flex_model[\"criterion\"] = torch.nn.CrossEntropyLoss()\n",
    "    server_flex_model[\"optimizer_func\"] = torch.optim.Adam\n",
    "    server_flex_model[\"optimizer_kwargs\"] = {}\n",
    "\n",
    "    return server_flex_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've defined the function to initialize the server model, we can create the FlexPool using the function `client_server_architecture`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flex_pool = FlexPool.client_server_architecture(fed_dataset=flex_dataset, init_func=build_server_model)\n",
    "\n",
    "clients = flex_pool.clients\n",
    "servers = flex_pool.servers\n",
    "aggregators = flex_pool.aggregators\n",
    "\n",
    "print(f\"Number of nodes in the pool {len(flex_pool)}: {len(servers)} server plus {len(clients)} clients. The server is also an aggregator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the decorator `deploy_server_model` to create a custom function that deploys our server model, or we can use the primitive `deploy_server_model_pt` to deploy the server model to the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import deploy_server_model, deploy_server_model_pt\n",
    "\n",
    "@deploy_server_model\n",
    "def copy_server_model_to_clients(server_flex_model: FlexModel):\n",
    "    return deepcopy(server_flex_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "servers.map(copy_server_model_to_clients, clients) # Using the function created with the decorator\n",
    "# servers.map(deploy_server_model_pt, clients) # Using the primitive function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As text needs to be preprocessed and batched on the clients, we can do it on the train function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    return string.strip().lower()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return list(spacy_tokenizer(clean_str(text)))\n",
    "\n",
    "def convert_token_to_idx(text_tokenized):\n",
    "    return [vocabulary[token] if token in vocabulary else vocabulary['unk'] for token in text_tokenized]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(convert_token_to_idx(preprocess_text(_text)))\n",
    "        text_list.append(processed_text)\n",
    "    return torch.tensor(label_list), pad_sequence(text_list, padding_value=vocabulary['pad'], batch_first=True)\n",
    "\n",
    "def train(client_flex_model: FlexModel, client_data: Dataset):\n",
    "    X_data, y_data = client_data.to_list()\n",
    "    client_dataloader = DataLoader(client_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "    model = client_flex_model[\"model\"]\n",
    "    optimizer = client_flex_model['optimizer_func'](model.parameters(), **client_flex_model[\"optimizer_kwargs\"])\n",
    "    model = model.train()\n",
    "    model = model.to(device)\n",
    "    criterion = client_flex_model[\"criterion\"]\n",
    "    for _ in tqdm(range(NUM_EPOCHS)):\n",
    "        for labels, texts in client_dataloader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(texts)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients.map(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we have to aggregate the weights from the clients model in order to update the global model. To to so, we are going to use the primitive `collect_clients_weights_pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import collect_clients_weights_pt\n",
    "\n",
    "aggregators.map(collect_clients_weights_pt, clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the weights are aggregated, we aggregate them. In this notebook we use the FedAvg method that is already implemented in FLEXible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import fed_avg\n",
    "\n",
    "aggregators.map(fed_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `set_aggregated_weights_pt` sed the aggregated weights to the server model to update it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import set_aggregated_weights_pt\n",
    "\n",
    "aggregators.map(set_aggregated_weights_pt, servers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's turn to evaluate the global model. To do so, we have to create a function using the decoratod `evaluate_server_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import evaluate_server_model\n",
    "\n",
    "@evaluate_server_model\n",
    "def evaluate_global_model(server_flex_model: FlexModel, test_data=None):\n",
    "    model = server_flex_model[\"model\"]\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    total_count = 0\n",
    "    model = model.to(device)\n",
    "    criterion=server_flex_model['criterion']\n",
    "    # get test data as a torchvision object\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=True, pin_memory=False, collate_fn=collate_batch)\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for target, data in test_dataloader:\n",
    "            total_count += target.size(0)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            losses.append(criterion(output, target).item())\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            test_acc += pred.eq(target.data.view_as(pred)).long().cpu().sum().item()\n",
    "\n",
    "    test_loss = sum(losses) / len(losses)\n",
    "    test_acc /= total_count\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = servers.map(evaluate_global_model, test_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flexible",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
