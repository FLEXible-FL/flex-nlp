{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "from datasets.load import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb_dataset = load_dataset('imdb', split=['train', 'test']) # Get the dataset from huggingface library\n",
    "train_imdb_dataset, test_imdb_dataset = torchtext.datasets.IMDB() # Get the dataset from torchtext library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparativos como los embeddings, el vocabulario, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import GloVe, FastText, vocab\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/wiki.en.vec: 6.60GB [05:04, 21.7MB/s]                                \n",
      "  0%|          | 0/2519370 [00:00<?, ?it/s]Skipping token b'2519370' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|██████████| 2519370/2519370 [02:07<00:00, 19766.43it/s]\n"
     ]
    }
   ],
   "source": [
    "glove = GloVe(name='6B', dim=100)\n",
    "fasttext = FastText(language='en')\n",
    "vocabulary = vocab(glove.stoi)\n",
    "vocabulary_fasttext = vocab(fasttext.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400000, 100])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocabulary)\n",
    "print(glove.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\", 'for']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.get_itos()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"This is an example sentence to test the tokenizer.\"\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "spacy_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "example_tokens = tokenizer(example)\n",
    "example_tokens_spacy = spacy_tokenizer(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic English Tokenizer: ['this', 'is', 'an', 'example', 'sentence', 'to', 'test', 'the', 'tokenizer', '.']\n",
      "Spacy Tokenizer: ['This', 'is', 'an', 'example', 'sentence', 'to', 'test', 'the', 'tokenizer', '.']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Basic English Tokenizer: {example_tokens}\")\n",
    "print(f\"Spacy Tokenizer: {example_tokens_spacy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(example_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tokens: ['This', 'is', 'an', 'example', 'sentence', 'to', 'test', 'the', 'tokenizer', '.']\n",
      "Example tokens: ['this', 'example', 'sentence', 'test', 'tokenizer', '.']\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(f\"Example tokens: {example_tokens_spacy}\")\n",
    "\n",
    "example_tokens_spacy = [word.lower() for word in example_tokens_spacy if word not in stop_words]\n",
    "\n",
    "print(f\"Example tokens: {example_tokens_spacy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "656502"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_fasttext['tokenizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Token This not found and default index is not set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/albertoargentedelcastillogarrido/Documents/UGR-Work/ArticuloTesis/flex-nlp/notebooks/Federated IMDb PT using FLExible with a LSTM.ipynb Celda 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/albertoargentedelcastillogarrido/Documents/UGR-Work/ArticuloTesis/flex-nlp/notebooks/Federated%20IMDb%20PT%20using%20FLExible%20with%20a%20LSTM.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m vocabulary_fasttext[\u001b[39m'\u001b[39;49m\u001b[39mThis\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "File \u001b[0;32m~/miniforge3/envs/flexible/lib/python3.11/site-packages/torchtext/vocab/vocab.py:65\u001b[0m, in \u001b[0;36mVocab.__getitem__\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mexport\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, token: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m     58\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m        token: The token used to lookup the corresponding index.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39m        The index corresponding to the associated token.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab[token]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Token This not found and default index is not set"
     ]
    }
   ],
   "source": [
    "vocabulary_fasttext['This']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_fasttext['this']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'This'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/albertoargentedelcastillogarrido/Documents/UGR-Work/ArticuloTesis/flex-nlp/notebooks/Federated IMDb PT using FLExible with a LSTM.ipynb Celda 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/albertoargentedelcastillogarrido/Documents/UGR-Work/ArticuloTesis/flex-nlp/notebooks/Federated%20IMDb%20PT%20using%20FLExible%20with%20a%20LSTM.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m vocabulary\u001b[39m.\u001b[39;49mget_stoi()[\u001b[39m'\u001b[39;49m\u001b[39mThis\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'This'"
     ]
    }
   ],
   "source": [
    "vocabulary.get_stoi()['This']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From centralized data to federated data\n",
    "\n",
    "First we're going to federate the dataset using the FedDataDristibution class, that has functions to load multiple datasets from deep learning libraries such as PyTorch or TensorFlow. In this notebook we are using PyTorch, so we need to use the functions from the PyTorch ecosystem, and for the text datasets, we need to use the function `from_config_with_torchtext_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/albertoargentedelcastillogarrido/Documents/FLEX-framework/flex/data/dataset.py:284: RuntimeWarning: The input dataset and arguments are not explicitly supported, therefore they might not work as expected.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from flex.data import FedDatasetConfig, FedDataDistribution\n",
    "\n",
    "config = FedDatasetConfig(seed=0)\n",
    "config.n_clients = 2\n",
    "config.replacement = False # ensure that clients do not share any data\n",
    "config.client_names = ['client1', 'client2'] # Optional\n",
    "flex_dataset = FedDataDistribution.from_config_with_torchtext_dataset(data=train_imdb_dataset, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also want to use the FLEXible dataset for the test data, so we just use da function `from_torchtext_dataset` in the Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/albertoargentedelcastillogarrido/Documents/FLEX-framework/flex/data/dataset.py:284: RuntimeWarning: The input dataset and arguments are not explicitly supported, therefore they might not work as expected.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from flex.data import Dataset\n",
    "\n",
    "test_dataset = Dataset.from_torchtext_dataset(test_imdb_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Federate a model with FLEXible.\n",
    "\n",
    "Once we've federated the dataset, it's time to create the FlexPool. The FlexPool class is the one that simulates the real-time scenario for federated learning, so it is in charge of the communications across actors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.model import FlexModel\n",
    "from flex.pool import FlexPool\n",
    "\n",
    "from flex.pool.decorators import init_server_model\n",
    "from flex.pool.decorators import deploy_server_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to simulate a client-server architecture, which we can easily build using the FlexPool class, using the function `client_server_architecture`. This function needs a FlexDataset, which we already have prepared, and a function to initialize the server model, which we have to create.\n",
    "\n",
    "The model we are going to use is a simple LSTM, which will have the embeddings, the LSTM, a Linear layer and the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes, emb_vectors) -> None:\n",
    "        super().__init__()\n",
    "        # self.emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embeddings_dim)\n",
    "        # self.emb.weight.data.copy_(emb_vectors) # Initialize the embedding layer with the pretrained embeddings\n",
    "        # self.emb.requires_grad_(False) # Freeze the embedding layer\n",
    "        # We can do the 3 steps above with the following line\n",
    "        self.emb = nn.Embedding.from_pretrained(embeddings=emb_vectors, freeze=True)\n",
    "        embeddings_dim = emb_vectors.shape[1]\n",
    "        self.lstm = nn.LSTM(input_size=embeddings_dim, hidden_size=hidden_size, num_layers=1)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.fc2 = nn.Linear(hidden_size//2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "@init_server_model\n",
    "def build_server_model():\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flexible",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
