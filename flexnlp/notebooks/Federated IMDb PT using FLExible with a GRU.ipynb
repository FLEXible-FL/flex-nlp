{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "from datasets.load import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import torchtext\n",
    "\n",
    "from flexnlp.utils.collators import ClassificationCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb_dataset = load_dataset('imdb', split=['train', 'test']) # Get the dataset from huggingface library\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS() # Get the dataset from torchtext library\n",
    "unique_classes = set([label for (label, text) in train_dataset])\n",
    "num_classes = len(unique_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparativos como los embeddings, el vocabulario, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import GloVe, FastText, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 50 # Dimension of the embeddings\n",
    "glove = GloVe(name='6B', dim=embeddings_dim) # Load GloVe embeddings with 100 dimensions.\n",
    "# fasttext = FastText(language='en') # To use FastText instead of GloVe\n",
    "vocabulary = vocab(glove.stoi)\n",
    "# vocabulary_fasttext = vocab(fasttext.stoi) # To use FastText instead of GloVe\n",
    "vocab_size = len(vocabulary) # Get the vocabulary size\n",
    "print(f\"Total vocabulary size: {vocab_size}\")\n",
    "print(f\"Shape of embeddings: {glove.vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"This is an example sentence to test the tokenizer.\"\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "spacy_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "example_tokens = tokenizer(example)\n",
    "example_tokens_spacy = spacy_tokenizer(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary.get_itos()[:10] # Get the first 10 words of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Padding token idx, pad: {vocabulary.get_itos()[0]}\") # Get the index of the word '<pad>' for padding\n",
    "print(f\"Padding token idx, pad: {vocabulary.get_itos()[0:10]}\") # Get the index of the word '<pad>' for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = \"<pad>\"\n",
    "pad_index = 0\n",
    "vocabulary.insert_token(pad_token, pad_index)\n",
    "vocabulary.set_default_index(pad_index)\n",
    "# glove.vectors = torch.cat(1, (torch.zeros(1, embeddings_dim), glove.vectors))\n",
    "pretrained_embeddings = glove.vectors\n",
    "print(f\"Len pretrained embeddings: {len(pretrained_embeddings)}\")\n",
    "pretrained_embeddings = torch.cat((torch.zeros(1,pretrained_embeddings.shape[1]),pretrained_embeddings))\n",
    "print(f\"Len pretrained embeddings: {len(pretrained_embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Padding token idx, pad: {vocabulary.get_itos()[0:10]}\") # Get the index of the word '<pad>' for padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the basic english tokenizer from PyTorch, or the SpaCy tokenizer if we have spacy downloaded. Here we probe both tokenizer with the same example sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Basic English Tokenizer: {example_tokens}\")\n",
    "print(f\"Spacy Tokenizer: {example_tokens_spacy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Client's will probably want to delete the stopwords, optional, as the embeddings may have vectors for most of the stopwords. Here we show multiple options show the user must decide what he prefers to use. In this notebook we're going to use the first case, as it will have most information. In other case, we would use the last one, so at least we keep the most information we can. \n",
    "\n",
    "Later we will have to tokenize the clients data, and then we will add the padding to the sequences, and will convert the token to the index of the embedding matrix (ids)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(f\"Example tokens tokenized: {[word.lower() for word in example_tokens_spacy]}\")\n",
    "\n",
    "print(f\"Example tokens without stopwords: {[word.lower() for word in example_tokens_spacy if word not in stop_words]}\")\n",
    "\n",
    "print(f\"Example tokens without stopwords and word in vocabulary: {[word.lower() for word in example_tokens_spacy if word not in stop_words and word.lower() in vocabulary]}\")\n",
    "\n",
    "print(f\"Example tokens without quitting stopwords and word in vocabulary: {[word.lower() for word in example_tokens_spacy if word.lower() in vocabulary]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From centralized data to federated data\n",
    "\n",
    "First we're going to federate the dataset using the FedDataDristibution class, that has functions to load multiple datasets from deep learning libraries such as PyTorch or TensorFlow. In this notebook we are using PyTorch, so we need to use the functions from the PyTorch ecosystem, and for the text datasets, we need to use the function `from_config_with_torchtext_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.data import FedDatasetConfig, FedDataDistribution\n",
    "\n",
    "config = FedDatasetConfig(seed=0)\n",
    "config.n_clients = 2\n",
    "config.replacement = False # ensure that clients do not share any data\n",
    "config.client_names = ['client1', 'client2'] # Optional\n",
    "flex_dataset = FedDataDistribution.from_config_with_torchtext_dataset(data=train_dataset, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also want to use the FLEXible dataset for the test data, so we just use da function `from_torchtext_dataset` in the Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.data import Dataset\n",
    "\n",
    "test_dataset = Dataset.from_torchtext_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Federate a model with FLEXible.\n",
    "\n",
    "Once we've federated the dataset, it's time to create the FlexPool. The FlexPool class is the one that simulates the real-time scenario for federated learning, so it is in charge of the communications across actors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.model import FlexModel\n",
    "from flex.pool import FlexPool\n",
    "\n",
    "from flex.pool.decorators import init_server_model\n",
    "from flex.pool.decorators import deploy_server_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to simulate a client-server architecture, which we can easily build using the FlexPool class, using the function `client_server_architecture`. This function needs a FlexDataset, which we already have prepared, and a function to initialize the server model, which we have to create.\n",
    "\n",
    "The model we are going to use is a simple LSTM, which will have the embeddings, the LSTM, a Linear layer and the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GruNet(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        # Initialize the Embedding Layer with the GloVe embeddings.\n",
    "        self.emb = nn.Embedding.from_pretrained(embeddings,\n",
    "                                                freeze=True,\n",
    "                                                padding_idx=0\n",
    "                                                )\n",
    "        # Take the embeddings size from the embeddings vector.\n",
    "        self.embedding_size = embeddings.shape[1]\n",
    "        #Create the GRU layer with just one layer.\n",
    "        self.gru = nn.GRU(self.embedding_size,\n",
    "                        hidden_size,\n",
    "                        batch_first=True,\n",
    "                        num_layers=1\n",
    "                    )\n",
    "        # Create the prediction layer.\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = [batch_size, len]\n",
    "        x = self.emb(x)\n",
    "        # x.shape = [batch_size, len, emb_dim]\n",
    "        _, x = self.gru(x)\n",
    "        # x.shape = [1, batch_size, hid_dim]\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "@init_server_model\n",
    "def build_server_model():\n",
    "    server_flex_model = FlexModel()\n",
    "\n",
    "    server_flex_model['model'] = GruNet(embeddings=pretrained_embeddings, hidden_size=128,\n",
    "                                        num_classes=num_classes)\n",
    "    # Required to store this for later stages of the FL training process\n",
    "    server_flex_model[\"criterion\"] = torch.nn.CrossEntropyLoss()\n",
    "    server_flex_model[\"optimizer_func\"] = torch.optim.SGD\n",
    "    server_flex_model[\"optimizer_kwargs\"] = {}\n",
    "\n",
    "    return server_flex_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've defined the function to initialize the server model, we can create the FlexPool using the function `client_server_architecture`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flex_pool = FlexPool.client_server_pool(fed_dataset=flex_dataset, init_func=build_server_model)\n",
    "\n",
    "clients = flex_pool.clients\n",
    "servers = flex_pool.servers\n",
    "aggregators = flex_pool.aggregators\n",
    "\n",
    "print(f\"Number of nodes in the pool {len(flex_pool)}: {len(servers)} server plus {len(clients)} clients. The server is also an aggregator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the decorator `deploy_server_model` to create a custom function that deploys our server model, or we can use the primitive `deploy_server_model_pt` to deploy the server model to the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import deploy_server_model, deploy_server_model_pt\n",
    "\n",
    "@deploy_server_model\n",
    "def copy_server_model_to_clients(server_flex_model: FlexModel):\n",
    "    return deepcopy(server_flex_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "servers.map(copy_server_model_to_clients, clients) # Using the function created with the decorator\n",
    "# servers.map(deploy_server_model_pt, clients) # Using the primitive function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As text needs to be preprocessed and batched on the clients, we can do it on the train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "\n",
    "    return string.strip().lower()\n",
    "\n",
    "def collate_batch(batch):\n",
    "    def preprocess_text(text):\n",
    "        text_transform = lambda x: [vocabulary[\"<pad>\"]]+[vocabulary[token] for token in spacy_tokenizer(x)]+[vocabulary[\"<pad>\"]]\n",
    "        return list(text_transform(clean_str(text)))\n",
    "    label_list, text_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        label_transform = lambda x: int(x) - 1\n",
    "        label_list.append(label_transform(_label))\n",
    "        processed_text = torch.tensor(preprocess_text(_text))\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    return pad_sequence(text_list, padding_value=pad_index, batch_first=True), label_list\n",
    "\n",
    "def batch_sampler_v2(batch_size, indices):\n",
    "    random.shuffle(indices)\n",
    "    pooled_indices = []\n",
    "    # create pool of indices with similar lengths \n",
    "    for i in range(0, len(indices), batch_size * 100):\n",
    "        pooled_indices.extend(sorted(indices[i:i + batch_size * 100], key=lambda x: x[1]))\n",
    "\n",
    "    pooled_indices = [x[0] for x in pooled_indices]\n",
    "\n",
    "    # yield indices for current batch\n",
    "    for i in range(0, len(pooled_indices), batch_size):\n",
    "        yield pooled_indices[i:i + batch_size]\n",
    "\n",
    "def train(client_flex_model: FlexModel, client_data: Dataset):\n",
    "    X_data, y_data = client_data.to_list()\n",
    "    if 'train_indices' not in client_flex_model:\n",
    "        train_indices = [(i, len(tokenizer(s[0]))) for i, s in enumerate(X_data)]\n",
    "        client_flex_model['train_indices'] = train_indices\n",
    "    else:\n",
    "        train_indices = client_flex_model['train_indices']\n",
    "    # batch_size=BATCH_SIZE, shuffle=True, # No es necesario usarlo porque usamos el batch_sampler\n",
    "    client_dataloader = DataLoader(client_data, collate_fn=collate_batch, batch_size=BATCH_SIZE,\n",
    "                                    shuffle=True)\n",
    "    #                             batch_sampler=batch_sampler_v2(BATCH_SIZE, train_indices))\n",
    "    model = client_flex_model[\"model\"]\n",
    "    # lr = 0.001\n",
    "    optimizer = client_flex_model['optimizer_func'](model.parameters(), lr=0.1, **client_flex_model[\"optimizer_kwargs\"])\n",
    "    model = model.train()\n",
    "    model = model.to(device)\n",
    "    criterion = client_flex_model[\"criterion\"]\n",
    "    # Al usar batch_sampler, hay que recargar el DataLoader en cada epoch.\n",
    "    for _ in tqdm(range(NUM_EPOCHS)):\n",
    "        # client_dataloader = DataLoader(client_data, collate_fn=collate_batch,\n",
    "        #                             batch_sampler=batch_sampler_v2(BATCH_SIZE, train_indices))\n",
    "        losses = []\n",
    "        total_acc, total_count = 0, 0\n",
    "        for texts, labels in client_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            predicted_labels = model(texts).squeeze(dim=0)\n",
    "            # pred = pred.squeeze(dim=0)\n",
    "            loss = criterion(predicted_labels, labels)\n",
    "            if predicted_labels.isnan().any():\n",
    "                print(f\"Text in batch: {texts}\")\n",
    "                print(f\"Predicted labels in batch: {predicted_labels}\")\n",
    "                print(f\"Labels in batch: {labels}\")\n",
    "                print(f\"Loss in batch: {loss}\")\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            total_acc += (predicted_labels.argmax(1) == labels).sum().item()\n",
    "            total_count += labels.shape[0]\n",
    "        total_loss = sum(losses)/len(losses)\n",
    "        total_acc = total_acc/total_count\n",
    "        print(f\"Accuracy after epoch: {total_acc}\\t|\\tLoss after epoch: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients.map(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we have to aggregate the weights from the clients model in order to update the global model. To to so, we are going to use the primitive `collect_clients_weights_pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import collect_clients_weights_pt\n",
    "\n",
    "aggregators.map(collect_clients_weights_pt, clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the weights are aggregated, we aggregate them. In this notebook we use the FedAvg method that is already implemented in FLEXible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import fed_avg\n",
    "\n",
    "aggregators.map(fed_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `set_aggregated_weights_pt` sed the aggregated weights to the server model to update it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import set_aggregated_weights_pt\n",
    "\n",
    "aggregators.map(set_aggregated_weights_pt, servers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's turn to evaluate the global model. To do so, we have to create a function using the decoratod `evaluate_server_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flex.pool import evaluate_server_model\n",
    "\n",
    "@evaluate_server_model\n",
    "def evaluate_global_model(server_flex_model: FlexModel, test_data=None):\n",
    "    model = server_flex_model[\"model\"]\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    total_count = 0\n",
    "    model = model.to(device)\n",
    "    criterion=server_flex_model['criterion']\n",
    "    # get test data as a torchvision object\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=True, pin_memory=False, collate_fn=collate_batch)\n",
    "    X_data, _ = test_dataset.to_list()\n",
    "    test_indices = [(i, len(tokenizer(s[0]))) for i, s in enumerate(X_data)]\n",
    "    test_dataloader = DataLoader(test_dataset, collate_fn=collate_batch,\n",
    "                                    batch_sampler=batch_sampler_v2(BATCH_SIZE, test_indices))\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            total_count += target.size(0)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data).squeeze(dim=0)\n",
    "            loss = criterion(output, target)\n",
    "            losses.append(loss.item())\n",
    "            test_acc += (output.argmax(1) == target).sum().item()\n",
    "            total_count += target.shape[0]\n",
    "            # print(f\"Prediciones: {pred.squeeze(dim=1)}\")\n",
    "            # print(f\"Output: {output.data.max(1, keepdim=True)}\")\n",
    "            # print(f\"Target: {target}\")\n",
    "            # print(pred.eq(target.data.view_as(pred)).long().cpu().sum().item())\n",
    "            # test_acc += pred.eq(target.data.view_as(pred)).long().cpu().sum().item()\n",
    "            # print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "    test_loss = sum(losses) / len(losses)\n",
    "    test_acc /= total_count\n",
    "    print(f\"test loss: {test_loss}\")\n",
    "    print(f\"test acc: {test_acc}\")\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = servers.map(evaluate_global_model, test_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the federated learning experiment for a few rounds\n",
    "\n",
    "Now, we can summarize the steps provided above and run the federated experiment for multiple rounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_n_rounds(n_rounds, clients_per_round=2):  \n",
    "    pool = FlexPool.client_server_pool(fed_dataset=flex_dataset, init_func=build_server_model)\n",
    "    for i in range(n_rounds):\n",
    "        print(f\"\\nRunning round: {i+1} of {n_rounds}\")\n",
    "        selected_clients_pool = pool.clients.select(clients_per_round)\n",
    "        selected_clients = selected_clients_pool.clients\n",
    "        print(f\"Selected clients for this round: {len(selected_clients)}\")\n",
    "        # Deploy the server model to the selected clients\n",
    "        pool.servers.map(deploy_server_model_pt, selected_clients)\n",
    "        # Each selected client trains her model\n",
    "        selected_clients.map(train)\n",
    "        # The aggregador collects weights from the selected clients and aggregates them\n",
    "        pool.aggregators.map(collect_clients_weights_pt, selected_clients)\n",
    "        pool.aggregators.map(fed_avg)\n",
    "        # The aggregator send its aggregated weights to the server\n",
    "        pool.aggregators.map(set_aggregated_weights_pt, pool.servers)\n",
    "        metrics = pool.servers.map(evaluate_global_model, test_data=test_imdb_dataset)\n",
    "        loss, acc = metrics[0]\n",
    "        print(f\"Server: Test acc: {acc:.4f}, test loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_n_rounds(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flexible",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
